# 初学者机器学习：神经网络简介

#### 简单解释它们如何工作以及如何在Python中从头开始实现一个。

**2019年3月3日**

这可能会给你带来惊喜：**神经网络并不复杂**！术语“神经网络”被广泛用作流行语，但实际上它们通常比人们想象的要简单得多。

**本文仅供初学者使用，并假设ZERO具有机器学习的先验知识**。我们将理解神经网络如何在Python中从头开始实现。

让我们开始吧！

## 1. 构建基块：神经元

首先，我们要讨论神经元，神经网络的基本单元。**神经元接受输入，对它们进行一些数学计算，并产生一个输出**。这是2输入神经元的样子：

![img](https://victorzhou.com/perceptron-a74a19dc0599aae11df7493c718abaf9.svg)

这里发生了3件事。首先，每个输入乘以一个权重：![1560694241841](/home/challow/.config/Typora/typora-user-images/1560694241841.png)
$$
x1​→x1​∗w1
$$

$$
x2​→x2​∗w2​
$$

接下来，所有加权输入添加偏差b：![1560694254310](/home/challow/.config/Typora/typora-user-images/1560694254310.png)
$$
(x1​∗w1​)+(x2​∗w2​)+b
$$
最后，总和通过激活函数传递：![1560694263987](/home/challow/.config/Typora/typora-user-images/1560694263987.png)
$$
y=f(x1​∗w1​+x2​∗w2​+b)
$$
激活函数用于将无界输入转换为具有良好，可预测形式的输出。常用的激活函数是sigmoid函数：

![1560694296586](/home/challow/.config/Typora/typora-user-images/1560694296586.png)

sigmoid函数仅输出范围（0,1）中的数字。您可以将其视为压缩（-∞，∞）到（0,1）（0,1） - 负无穷大为〜0 ，正无穷大为～1。

#### 一个简单的例子

假设我们有一个2输入神经元，它使用sigmoid激活函数并具有以下参数：
$$
w=[0,1] 
$$

$$
b=4
$$

w=[0,1] 只是w1=0,w2=1的一种向量写法。现在，让我们给神经元一个输入x=[2,3]。我们将使用点积来更简洁地写出：
$$
(w⋅x)+b​ =((w1​∗x1​)+(w2​∗x2​))+b  
\\      =0∗2+1∗3+4\\=7
$$

$$
y=f(w⋅x+b)=f(7)=0.999
$$
给定输入x=[2,3]神经元输出0.999。向前传递输入以获得输出的过程称为**前馈**。

#### 编写一个神经元

是时候实施一个神经元了！我们将使用NumPy，一个流行且功能强大的Python计算库来帮助我们进行数学运算：

```python
import numpy as np

def sigmoid(x):
  # Our activation function: f(x) = 1 / (1 + e^(-x))
  return 1 / (1 + np.exp(-x))

class Neuron:
  def __init__(self, weights, bias):
    self.weights = weights
    self.bias = bias

  def feedforward(self, inputs):
    # Weight inputs, add bias, then use the activation function
    total = np.dot(self.weights, inputs) + self.bias
    return sigmoid(total)

weights = np.array([0, 1]) # w1 = 0, w2 = 1
bias = 4                   # b = 4
n = Neuron(weights, bias)

x = np.array([2, 3])       # x1 = 2, x2 = 3
print(n.feedforward(x))    # 0.9990889488055994
```

认识到这些数字？这就是我们刚刚做的例子！我们得到0.999这个相同答案。

## 2.将神经元组合成神经网络

神经网络只不过是连接在一起的一堆神经元。这是一个简单的神经网络可能是什么样子：

![1560695057057](/home/challow/.config/Typora/typora-user-images/1560695057057.png)

该网络有2个输入，一个隐藏层，有2个神经元（h1和h2），和一个带有1个神经元的输出层（o1）。注意到o1是h1和h2的输出这才是网络形成的原因。

> **隐藏层**是输入（第一）层和输出（最后）层之间的任何层。可以有多个隐藏层！

#### 一个例子：前馈

让我们使用上面描绘的网络并假设所有神经元具有相同的权重w=[0,1]，相同的偏差b=0,相同的sigmoid激活函数。设h1，h2，o1表示它们代表的神经元的输出。

如果我们传入输入x=[2,3]会发生什么？

![1560695646334](/home/challow/.config/Typora/typora-user-images/1560695646334.png)

输入x=[2,3]的神经网络的输出是0.7216.很简单，对吗？

神经网络可以具有任意数量的层，这些层中具有任意数量的神经元。基本思想保持不变：通过网络中的神经元向前馈送输入以获得最后的输出。为简单起见，我们将继续使用上图所示的网络来完成本文的其余部分。

#### 编写神经网络：前馈

让我们为神经网络实现前馈。这是网络的图像再次供参考：

![1560695752202](/home/challow/.config/Typora/typora-user-images/1560695752202.png)

```python
import numpy as np

# ... code from previous section here

class OurNeuralNetwork:
  '''
  A neural network with:
    - 2 inputs
    - a hidden layer with 2 neurons (h1, h2)
    - an output layer with 1 neuron (o1)
  Each neuron has the same weights and bias:
    - w = [0, 1]
    - b = 0
  '''
  def __init__(self):
    weights = np.array([0, 1])
    bias = 0

    # The Neuron class here is from the previous section
    self.h1 = Neuron(weights, bias)
    self.h2 = Neuron(weights, bias)
    self.o1 = Neuron(weights, bias)

  def feedforward(self, x):
    out_h1 = self.h1.feedforward(x)
    out_h2 = self.h2.feedforward(x)

    # The inputs for o1 are the outputs from h1 and h2
    out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))

    return out_o1

network = OurNeuralNetwork()
x = np.array([2, 3])
print(network.feedforward(x)) # 0.7216325609518421
```

我们又得到了0.7216！看起来很有效。

## 3.训练神经网络，第1部分

假设我们有以下测量值：

|  Name   | Weight (lb) | Height (in) | Gender |
| :-----: | :---------: | :---------: | :----: |
|  Alice  |     133     |     65      |   F    |
|   Bob   |     160     |     72      |   M    |
| Charlie |     152     |     70      |   M    |
|  Diana  |     120     |     60      |   F    |

让我们训练我们的网络，根据他们的体重和身高来预测某人的性别：

![1560695886939](/home/challow/.config/Typora/typora-user-images/1560695886939.png)

我们将使用0代表男性和使用1代表女性，我们也将转化数据，使其更容易使用：

|  Name   | Weight (minus 135) | Height (minus 66) | Gender |
| :-----: | :----------------: | :---------------: | :----: |
|  Alice  |         -2         |        -1         |   1    |
|   Bob   |         25         |         6         |   0    |
| Charlie |         17         |         4         |   0    |
|  Diana  |        -15         |        -6         |   1    |

> 我任意的选择了转化的值(135,66)让数字看起来更舒服。一般情况下使用平均值即可。

#### 损失

在我们训练网络之前，我们首先需要一种方法来量化它的“好”程度，以便它可以尝试“更好”。这就是损失。

我们将使用均方误差（MSE）损失：

![1560696121743](/home/challow/.config/Typora/typora-user-images/1560696121743.png)

让我们拆分它：

- n是样本的数量

- y表示预测的变量，即性别。

- y_true是变量的真实值（“正确的答案”）例如，y_true对Alice来说是1

- y_pred是变量的预测值。我们神经网络的输出。

（y_true - y_pred）的平方值是所熟悉的平方误差。我们的损失函数只是取所有平方误差的平均值（因此名称均方误差）。我们的预测越好，我们的损失就越低！

更好的预测=更低的损失。

**培训网络=尽量减少损失。**

#### 一个误差计算的示例

假设我们的网络总是输出0  - 换句话说，它确信所有人都是男性 🤔 我们的损失是什么？

| Name    | y_true | y_pred | (y_true − y_pred)^2 |
| ------- | :----: | :----: | :-----------------: |
| Alice   |   1    |   0    |          1          |
| Bob     |   0    |   0    |          0          |
| Charlie |   0    |   0    |          0          |
| Diana   |   1    |   0    |          1          |

![1560696540866](/home/challow/.config/Typora/typora-user-images/1560696540866.png)

#### 代码：MSE损失

以下是为我们计算损失的一些代码：

```python
import numpy as np

def mse_loss(y_true, y_pred):
  # y_true and y_pred are numpy arrays of the same length.
  return ((y_true - y_pred) ** 2).mean()

y_true = np.array([1, 0, 0, 1])
y_pred = np.array([0, 0, 0, 0])

print(mse_loss(y_true, y_pred)) # 0.5
```

