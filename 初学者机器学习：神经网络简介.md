# 初学者机器学习：神经网络简介

#### 简单解释它们如何工作以及如何在Python中从头开始实现一个。

**2019年3月3日**

这可能会给你带来惊喜：**神经网络并不复杂**！术语“神经网络”被广泛用作流行语，但实际上它们通常比人们想象的要简单得多。

**本文仅供初学者使用，并假设ZERO具有机器学习的先验知识**。我们将理解神经网络如何在Python中从头开始实现。

让我们开始吧！

## 1. 构建基块：神经元

首先，我们要讨论神经元，神经网络的基本单元。**神经元接受输入，对它们进行一些数学计算，并产生一个输出**。这是2输入神经元的样子：

![img](https://victorzhou.com/perceptron-a74a19dc0599aae11df7493c718abaf9.svg)

这里发生了3件事。首先，每个输入乘以一个权重：![1560694241841](https://github.com/challow0/build_nn/blob/master/image/%E9%80%89%E5%8C%BA_001.png)
$$
x1​→x1​∗w1
$$

$$
x2​→x2​∗w2​
$$

接下来，所有加权输入添加偏差b：![1560694254310](https://github.com/challow0/build_nn/blob/master/image/%E9%80%89%E5%8C%BA_002.png)
$$
(x1​∗w1​)+(x2​∗w2​)+b
$$
最后，总和通过激活函数传递：![1560694263987](https://github.com/challow0/build_nn/blob/master/image/%E9%80%89%E5%8C%BA_003.png)
$$
y=f(x1​∗w1​+x2​∗w2​+b)
$$
激活函数用于将无界输入转换为具有良好，可预测形式的输出。常用的激活函数是sigmoid函数：

![1560694296586](https://github.com/challow0/build_nn/blob/master/image/%E9%80%89%E5%8C%BA_004.png)

sigmoid函数仅输出范围（0,1）中的数字。您可以将其视为压缩（-∞，∞）到（0,1）（0,1） - 负无穷大为〜0 ，正无穷大为～1。

#### 一个简单的例子

假设我们有一个2输入神经元，它使用sigmoid激活函数并具有以下参数：
$$
w=[0,1] 
$$

$$
b=4
$$

w=[0,1] 只是w1=0,w2=1的一种向量写法。现在，让我们给神经元一个输入x=[2,3]。我们将使用点积来更简洁地写出：
$$
(w⋅x)+b​ =((w1​∗x1​)+(w2​∗x2​))+b  
\\      =0∗2+1∗3+4\\=7
$$

$$
y=f(w⋅x+b)=f(7)=0.999
$$
给定输入x=[2,3]神经元输出0.999。向前传递输入以获得输出的过程称为**前馈**。

#### 编写一个神经元

是时候实施一个神经元了！我们将使用NumPy，一个流行且功能强大的Python计算库来帮助我们进行数学运算：

```python
import numpy as np

def sigmoid(x):
  # Our activation function: f(x) = 1 / (1 + e^(-x))
  return 1 / (1 + np.exp(-x))

class Neuron:
  def __init__(self, weights, bias):
    self.weights = weights
    self.bias = bias

  def feedforward(self, inputs):
    # Weight inputs, add bias, then use the activation function
    total = np.dot(self.weights, inputs) + self.bias
    return sigmoid(total)

weights = np.array([0, 1]) # w1 = 0, w2 = 1
bias = 4                   # b = 4
n = Neuron(weights, bias)

x = np.array([2, 3])       # x1 = 2, x2 = 3
print(n.feedforward(x))    # 0.9990889488055994
```

认识到这些数字？这就是我们刚刚做的例子！我们得到0.999这个相同答案。

## 2.将神经元组合成神经网络

神经网络只不过是连接在一起的一堆神经元。这是一个简单的神经网络可能是什么样子：

![1560695057057](https://github.com/challow0/build_nn/blob/master/image/%E9%80%89%E5%8C%BA_005.png)

该网络有2个输入，一个隐藏层，有2个神经元（h1和h2），和一个带有1个神经元的输出层（o1）。注意到o1是h1和h2的输出这才是网络形成的原因。

> **隐藏层**是输入（第一）层和输出（最后）层之间的任何层。可以有多个隐藏层！

#### 一个例子：前馈

让我们使用上面描绘的网络并假设所有神经元具有相同的权重w=[0,1]，相同的偏差b=0,相同的sigmoid激活函数。设h1，h2，o1表示它们代表的神经元的输出。

如果我们传入输入x=[2,3]会发生什么？

![1560695646334](https://github.com/challow0/build_nn/blob/master/image/%E9%80%89%E5%8C%BA_006.png)

输入x=[2,3]的神经网络的输出是0.7216.很简单，对吗？

神经网络可以具有任意数量的层，这些层中具有任意数量的神经元。基本思想保持不变：通过网络中的神经元向前馈送输入以获得最后的输出。为简单起见，我们将继续使用上图所示的网络来完成本文的其余部分。

#### 编写神经网络：前馈

让我们为神经网络实现前馈。这是网络的图像再次供参考：

![1560695752202](https://github.com/challow0/build_nn/blob/master/image/%E9%80%89%E5%8C%BA_007.png)

```python
import numpy as np

# ... code from previous section here

class OurNeuralNetwork:
  '''
  A neural network with:
    - 2 inputs
    - a hidden layer with 2 neurons (h1, h2)
    - an output layer with 1 neuron (o1)
  Each neuron has the same weights and bias:
    - w = [0, 1]
    - b = 0
  '''
  def __init__(self):
    weights = np.array([0, 1])
    bias = 0

    # The Neuron class here is from the previous section
    self.h1 = Neuron(weights, bias)
    self.h2 = Neuron(weights, bias)
    self.o1 = Neuron(weights, bias)

  def feedforward(self, x):
    out_h1 = self.h1.feedforward(x)
    out_h2 = self.h2.feedforward(x)

    # The inputs for o1 are the outputs from h1 and h2
    out_o1 = self.o1.feedforward(np.array([out_h1, out_h2]))

    return out_o1

network = OurNeuralNetwork()
x = np.array([2, 3])
print(network.feedforward(x)) # 0.7216325609518421
```

我们又得到了0.7216！看起来很有效。

## 3.训练神经网络，第1部分

假设我们有以下测量值：

|  Name   | Weight (lb) | Height (in) | Gender |
| :-----: | :---------: | :---------: | :----: |
|  Alice  |     133     |     65      |   F    |
|   Bob   |     160     |     72      |   M    |
| Charlie |     152     |     70      |   M    |
|  Diana  |     120     |     60      |   F    |

让我们训练我们的网络，根据他们的体重和身高来预测某人的性别：

![1560695886939](https://github.com/challow0/build_nn/blob/master/image/%E9%80%89%E5%8C%BA_008.png)

我们将使用0代表男性和使用1代表女性，我们也将转化数据，使其更容易使用：

|  Name   | Weight (minus 135) | Height (minus 66) | Gender |
| :-----: | :----------------: | :---------------: | :----: |
|  Alice  |         -2         |        -1         |   1    |
|   Bob   |         25         |         6         |   0    |
| Charlie |         17         |         4         |   0    |
|  Diana  |        -15         |        -6         |   1    |

> 我任意的选择了转化的值(135,66)让数字看起来更舒服。一般情况下使用平均值即可。

#### 损失

在我们训练网络之前，我们首先需要一种方法来量化它的“好”程度，以便它可以尝试“更好”。这就是损失。

我们将使用均方误差（MSE）损失：

![1560696121743](https://github.com/challow0/build_nn/blob/master/image/%E9%80%89%E5%8C%BA_009.png)

让我们拆分它：

- n是样本的数量

- y表示预测的变量，即性别。

- y_true是变量的真实值（“正确的答案”）例如，y_true对Alice来说是1

- y_pred是变量的预测值。我们神经网络的输出。

（y_true - y_pred）的平方值是所熟悉的平方误差。我们的损失函数只是取所有平方误差的平均值（因此名称均方误差）。我们的预测越好，我们的损失就越低！

更好的预测=更低的损失。

**培训网络=尽量减少损失。**

#### 一个误差计算的示例

假设我们的网络总是输出0  - 换句话说，它确信所有人都是男性 🤔 我们的损失是什么？

| Name    | y_true | y_pred | (y_true − y_pred)^2 |
| ------- | :----: | :----: | :-----------------: |
| Alice   |   1    |   0    |          1          |
| Bob     |   0    |   0    |          0          |
| Charlie |   0    |   0    |          0          |
| Diana   |   1    |   0    |          1          |

![1560696540866](https://github.com/challow0/build_nn/blob/master/image/%E9%80%89%E5%8C%BA_010.png)

#### 代码：MSE损失

以下是为我们计算损失的一些代码：

```python
import numpy as np

def mse_loss(y_true, y_pred):
  # y_true and y_pred are numpy arrays of the same length.
  return ((y_true - y_pred) ** 2).mean()

y_true = np.array([1, 0, 0, 1])
y_pred = np.array([0, 0, 0, 0])

print(mse_loss(y_true, y_pred)) # 0.5
```

## 4.训练神经网络，第2部分

我们现在有一个明确的目标：尽量**减少神经网络的损失**。我们知道我们可以改变网络的权重和偏差以影响其预测，但我们如何以减少损失的方式这样做呢？

> 本节使用了一些多变量微积分。如果您对微积分不熟悉，可以随意跳过数学部分。

为简单起见，让我们假装我们的数据集中只有Alice：

| Name  | Weight (minus 135) | Height (minus 66) | Gender |
| :---: | :----------------: | :---------------: | :----: |
| Alice |         -2         |        -1         |   1    |

那么均方误差损失只是Alice的平方误差：

![1560785551167](https://github.com/challow0/build_nn/blob/master/image/%E9%80%89%E5%8C%BA_011.png)

考虑损失的另一种方式是权重和偏差。让我们在网络中标出每个权重和偏见：

![1560785571631](https://github.com/challow0/build_nn/blob/master/image/%E9%80%89%E5%8C%BA_012.png)

然后，我们可以将损失写为多变量函数：


$$
L(w1​,w2​,w3​,w4​,w5​,w6​,b1​,b2​,b3​)
$$
想象一下，我们想调整一下w1.如果我们改变w1，L会如何变化？这是一个问题，偏导数![1560785713654](https://github.com/challow0/build_nn/blob/master/image/%E9%80%89%E5%8C%BA_013.png)可以回答。我们如何计算呢？

> 这是数学开始变得更复杂的地方。不要气馁！我建议让笔和纸一起跟进 - 它会帮助你理解。

首先，让我们重写一下偏导数![1560785760740](https://github.com/challow0/build_nn/blob/master/image/%E9%80%89%E5%8C%BA_014.png)

![1560785773253](https://github.com/challow0/build_nn/blob/master/image/%E9%80%89%E5%8C%BA_015.png)

> 这是因为链式规则。

我们可以算一算![1560785839450](https://github.com/challow0/build_nn/blob/master/image/%E9%80%89%E5%8C%BA_016.png)因为我们算了![1560785875132](https://github.com/challow0/build_nn/blob/master/image/%E9%80%89%E5%8C%BA_017.png)

![1560785983462](https://github.com/challow0/build_nn/blob/master/image/%E9%80%89%E5%8C%BA_019.png)

现在，让我们弄清楚该怎么做![1560785897969](https://github.com/challow0/build_nn/blob/master/image/%E9%80%89%E5%8C%BA_018.png)就像以前那样让h1，h2，o1成为它们所代表的神经元的输出。然后：

![1560785997352](https://github.com/challow0/build_nn/blob/master/image/%E9%80%89%E5%8C%BA_020.png)

> f是sigmoid激活函数，还记得吗？

由于w1只影响h1（不是h2），我们可以写

![1560786054680](https://github.com/challow0/build_nn/blob/master/image/%E9%80%89%E5%8C%BA_021.png)

我们对![1560786085343](https://github.com/challow0/build_nn/blob/master/image/%E9%80%89%E5%8C%BA_022.png)做同样的事情：

![1560786099329](https://github.com/challow0/build_nn/blob/master/image/%E9%80%89%E5%8C%BA_023.png)

> 你猜到了，链式法则。。

x1这里是重量，x2是高度。这是我们第二次看到f'（x）（sigmoid函数的推导）！让我们推导出来：

![1560786178328](https://github.com/challow0/build_nn/blob/master/image/%E9%80%89%E5%8C%BA_024.png)

我们稍后会将这个漂亮的形式用于f'（x）。

我们完成了！我们设法将![1560786230260](https://github.com/challow0/build_nn/blob/master/image/%E9%80%89%E5%8C%BA_025.png)分解成我们可以计算的几个部分：

![1560786241274](https://github.com/challow0/build_nn/blob/master/image/%E9%80%89%E5%8C%BA_026.png)

这种通过向后工作计算偏导数的系统称为**反向传播**或“反向传播”。

唷。这是很多符号 - 如果你仍然有点困惑是没有问题的。让我们举个例子来看看这个行动吧！

#### 示例：计算偏导数

我们将继续假装只有Alice在我们的数据集中：

| Name  | Weight (minus 135) | Height (minus 66) | Gender |
| :---: | :----------------: | :---------------: | :----: |
| Alice |         -2         |        -1         |   1    |

让我们将所有权重初始化为1，将所有偏差初始化为0.如果我们通过网络进行前馈传递，我们得到：

![1560872675440](https://github.com/challow0/build_nn/blob/master/image/%E9%80%89%E5%8C%BA_027.png)

网络输出y_pred=0.524,不强烈支持男性（0）或女性（1）.我们算一算吧![1560872740444](https://github.com/challow0/build_nn/blob/master/image/%E9%80%89%E5%8C%BA_028.png):

![1560872768635](https://github.com/challow0/build_nn/blob/master/image/%E9%80%89%E5%8C%BA_029.png)

> 提醒：我们导出f'（x）= f（x）*（1  -  f（x））我们之前的sigmoid激活函数。

我们做到了！这告诉我们如果我们要增加w1，那么L会增加一个tiiiny位。

#### 训练：随机梯度下降

**我们现在拥有培训神经网络所需的所有工具！**我们将使用一种称为随机梯度下降（SGD）的优化算法，该算法告诉我们如何改变我们的权重和偏差以最小化损失。它基本上就是这个更新等式：

![1560872965542](https://github.com/challow0/build_nn/blob/master/image/%E9%80%89%E5%8C%BA_030.png)

η是一个常数，称为**学习率**，控制我们训练的速度。我们所做的就是从w1减去![1560873018590](https://github.com/challow0/build_nn/blob/master/image/%E9%80%89%E5%8C%BA_031.png)

- 如果![1560873076843](https://github.com/challow0/build_nn/blob/master/image/%E9%80%89%E5%8C%BA_032.png)是正的，w1就会减小，随之L也会减小
- 如果![1560873076843](https://github.com/challow0/build_nn/blob/master/image/%E9%80%89%E5%8C%BA_032.png)是负的，w1就会增加，随之L也会减小

如果我们针对网络中的每个重量和偏差做到这一点，那么损失将逐渐减少，我们的网络将会改善。

我们的训练流程如下：

1.从我们的数据集中选择一个样本。这就是随机梯度下降的原因 - 我们一次只对一个样本进行操作。

2.计算相对于权重或偏差的所有损失的偏导数

3.使用更新公式更新每个权重和偏差。

4.回到第1步。

让我们用行动看看！

#### 代码：完整的神经网络

现在终于实现了一个完整的神经网络：

|  Name   | Weight (minus 135) | Height (minus 66) | Gender |
| :-----: | :----------------: | :---------------: | :----: |
|  Alice  |         -2         |        -1         |   1    |
|   Bob   |         25         |         6         |   0    |
| Charlie |         17         |         4         |   0    |
|  Diana  |        -15         |        -6         |   1    |

![1560873280455](https://github.com/challow0/build_nn/blob/master/image/%E9%80%89%E5%8C%BA_033.png)

```python
import numpy as np

def sigmoid(x):
  # Sigmoid activation function: f(x) = 1 / (1 + e^(-x))
  return 1 / (1 + np.exp(-x))

def deriv_sigmoid(x):
  # Derivative of sigmoid: f'(x) = f(x) * (1 - f(x))
  fx = sigmoid(x)
  return fx * (1 - fx)

def mse_loss(y_true, y_pred):
  # y_true and y_pred are numpy arrays of the same length.
  return ((y_true - y_pred) ** 2).mean()

class OurNeuralNetwork:
  '''
  A neural network with:
    - 2 inputs
    - a hidden layer with 2 neurons (h1, h2)
    - an output layer with 1 neuron (o1)

  *** DISCLAIMER ***:
  The code below is intended to be simple and educational, NOT optimal.
  Real neural net code looks nothing like this. DO NOT use this code.
  Instead, read/run it to understand how this specific network works.
  '''
  def __init__(self):
    # Weights
    self.w1 = np.random.normal()
    self.w2 = np.random.normal()
    self.w3 = np.random.normal()
    self.w4 = np.random.normal()
    self.w5 = np.random.normal()
    self.w6 = np.random.normal()

    # Biases
    self.b1 = np.random.normal()
    self.b2 = np.random.normal()
    self.b3 = np.random.normal()

  def feedforward(self, x):
    # x is a numpy array with 2 elements.
    h1 = sigmoid(self.w1 * x[0] + self.w2 * x[1] + self.b1)
    h2 = sigmoid(self.w3 * x[0] + self.w4 * x[1] + self.b2)
    o1 = sigmoid(self.w5 * h1 + self.w6 * h2 + self.b3)
    return o1

  def train(self, data, all_y_trues):
    '''
    - data is a (n x 2) numpy array, n = # of samples in the dataset.
    - all_y_trues is a numpy array with n elements.
      Elements in all_y_trues correspond to those in data.
    '''
    learn_rate = 0.1
    epochs = 1000 # number of times to loop through the entire dataset

    for epoch in range(epochs):
      for x, y_true in zip(data, all_y_trues):
        # --- Do a feedforward (we'll need these values later)
        sum_h1 = self.w1 * x[0] + self.w2 * x[1] + self.b1
        h1 = sigmoid(sum_h1)

        sum_h2 = self.w3 * x[0] + self.w4 * x[1] + self.b2
        h2 = sigmoid(sum_h2)

        sum_o1 = self.w5 * h1 + self.w6 * h2 + self.b3
        o1 = sigmoid(sum_o1)
        y_pred = o1

        # --- Calculate partial derivatives.
        # --- Naming: d_L_d_w1 represents "partial L / partial w1"
        d_L_d_ypred = -2 * (y_true - y_pred)

        # Neuron o1
        d_ypred_d_w5 = h1 * deriv_sigmoid(sum_o1)
        d_ypred_d_w6 = h2 * deriv_sigmoid(sum_o1)
        d_ypred_d_b3 = deriv_sigmoid(sum_o1)

        d_ypred_d_h1 = self.w5 * deriv_sigmoid(sum_o1)
        d_ypred_d_h2 = self.w6 * deriv_sigmoid(sum_o1)

        # Neuron h1
        d_h1_d_w1 = x[0] * deriv_sigmoid(sum_h1)
        d_h1_d_w2 = x[1] * deriv_sigmoid(sum_h1)
        d_h1_d_b1 = deriv_sigmoid(sum_h1)

        # Neuron h2
        d_h2_d_w3 = x[0] * deriv_sigmoid(sum_h2)
        d_h2_d_w4 = x[1] * deriv_sigmoid(sum_h2)
        d_h2_d_b2 = deriv_sigmoid(sum_h2)

        # --- Update weights and biases
        # Neuron h1
        self.w1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w1
        self.w2 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_w2
        self.b1 -= learn_rate * d_L_d_ypred * d_ypred_d_h1 * d_h1_d_b1

        # Neuron h2
        self.w3 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w3
        self.w4 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_w4
        self.b2 -= learn_rate * d_L_d_ypred * d_ypred_d_h2 * d_h2_d_b2

        # Neuron o1
        self.w5 -= learn_rate * d_L_d_ypred * d_ypred_d_w5
        self.w6 -= learn_rate * d_L_d_ypred * d_ypred_d_w6
        self.b3 -= learn_rate * d_L_d_ypred * d_ypred_d_b3

      # --- Calculate total loss at the end of each epoch
      if epoch % 10 == 0:
        y_preds = np.apply_along_axis(self.feedforward, 1, data)
        loss = mse_loss(all_y_trues, y_preds)
        print("Epoch %d loss: %.3f" % (epoch, loss))

# Define dataset
data = np.array([
  [-2, -1],  # Alice
  [25, 6],   # Bob
  [17, 4],   # Charlie
  [-15, -6], # Diana
])
all_y_trues = np.array([
  1, # Alice
  0, # Bob
  0, # Charlie
  1, # Diana
])

# Train our neural network!
network = OurNeuralNetwork()
network.train(data, all_y_trues)
```

随着网络的学习，我们的损失稳步下降：

![1560873320410](https://github.com/challow0/build_nn/blob/master/image/%E9%80%89%E5%8C%BA_034.png)

我们现在可以使用网络来预测性别：

```python
# Make some predictions
emily = np.array([-7, -3]) # 128 pounds, 63 inches
frank = np.array([20, 2])  # 155 pounds, 68 inches
print("Emily: %.3f" % network.feedforward(emily)) # 0.951 - F
print("Frank: %.3f" % network.feedforward(frank)) # 0.039 - M
```

