#               11行Python代码的神经网络

---

​                                     **一个原生态的神经网络来描述反向传播算法的工作原理**

**摘要**：我可以通过玩具代码学到最好的东西。本教程通过一个非常简单的玩具示例，一个简短的python实现来教授反向传播。

### 请给我代码：

```python
X = np.array([ [0,0,1],[0,1,1],[1,0,1],[1,1,1] ])
y = np.array([[0,1,1,0]]).T
syn0 = 2*np.random.random((3,4)) - 1
syn1 = 2*np.random.random((4,1)) - 1
for j in xrange(60000):
    l1 = 1/(1+np.exp(-(np.dot(X,syn0))))
    l2 = 1/(1+np.exp(-(np.dot(l1,syn1))))
    l2_delta = (y - l2)*(l2*(1-l2))
    l1_delta = l2_delta.dot(syn1.T) * (l1 * (1-l1))
    syn1 += l1.T.dot(l2_delta)
    syn0 += X.T.dot(l1_delta)
```

但是，这有点简洁......让我们把它分成几个简单的部分。

## 第一部分：一个小小的玩具网络

用反向传播训练的神经网络试图使用输入来预测输出。

| Inputs |      |      | Output |
| :----: | :--: | :--: | :----: |
|   0    |  0   |  1   |   0    |
|   1    |  1   |  1   |   1    |
|   1    |  0   |  1   |   1    |
|   0    |  1   |  1   |   0    |

考虑尝试在给定三个输入列的情况下预测输出列。我们可以通过简单地**测量输入值和输出值之间的统计数据**来解决这个问题。如果我们这样做，我们会看到最左边的输入列与输出**完全相关**。

反向传播，以其最简单的形式，测量这样的统计数据来制作模型。让我们直接进入并使用它来做到这一点。

### 2 Layer Neural Network:

```python
import numpy as np

# sigmoid 函数
def nonlin(x,deriv=False):
    if(deriv==True):
        return x*(1-x)
    return 1/(1+np.exp(-x))
    
# 输入数据
X = np.array([  [0,0,1],
                [0,1,1],
                [1,0,1],
                [1,1,1] ])
    
# 输出数据            
y = np.array([[0,0,1,1]]).T

# seed随机数进行运算
# 确定性 (只是一个很好的做法)
np.random.seed(1)

# 用均值0随机初始化权重
syn0 = 2*np.random.random((3,1)) - 1

for iter in range(10000):

    # 前向传播
    l0 = X
    l1 = nonlin(np.dot(l0,syn0))

    # 我们丢失了多少？
    l1_error = y - l1

 	#乘以ll中的我们失去的sigmoid斜率
    
    l1_delta = l1_error * nonlin(l1,True)

    # 更新权重
    syn0 += np.dot(l0.T,l1_delta)

print ('输出我们训练后的结果：')
print (l1)
'''
Output After Training:
[[ 0.00966449]
 [ 0.00786506]
 [ 0.99358898]
 [ 0.99211957]]
'''

```

| 变量     | 定义                                                         |
| -------- | ------------------------------------------------------------ |
| X        | 输入数据集矩阵，其中每一行都是一个训练示例                   |
| y        | 输出数据集矩阵，其中每一行都是一个训练示例                   |
| l0       | 网络的第一层，由输入数据指定                                 |
| l1       | 网络的第二层，也称为隐藏层                                   |
| syn0     | 第一层权重，Synapse 0，连接 l0 ， l1。                       |
| *        | 元素乘法，因此两个相等大小的矢量将相应的值乘以1对1以生成相同大小的最终矢量。 |
| -        | 元素减法，因此两个相等大小的矢量减去相应的值1到1，以生成相同大小的最终矢量。 |
| x.dot(y) | 如果x和y是向量，则这是点积。如果两者都是矩阵，那么它就是矩阵乘法。如果只有一个是矩阵，则它是向量矩阵乘法。 |

正如您在“训练后输出”中所看到的，它奏效了！在我描述流程之前，我建议您使用代码来直观地了解它的工作原理。您应该能够在  ipython notebook中“按原样”运行它（如果必须，可以运行脚本，但我强烈推荐使用笔记本）。以下是一些要在代码中查看的好地方：

- 比较第一次迭代后和最后一次迭代后 l1 的值。

- 检查"nonlin"函数，这就是我们输出概率的原因。

- 查看l1_error在迭代时如何更改。

- 拆开第36行，大部分的奥秘在此处

- 检查第39行，网络中的所有内容都为此操作做好准备。

  

让我们逐行浏览代码。



**建议**：在两个屏幕中打开此博客，以便在阅读时看到代码。这就是我写这篇文章时的所作所为。 :)

**第01行**：导入numpy，这是一个线性代数库。这是我们唯一的依赖。

**第04行**：这是我们的“非线性”。虽然它可以是几种函数，但这种非线性映射了一种称为“sigmoid”的函数。 sigmoid函数将任何值映射到0到1之间的值。我们使用它将数字转换为概率。它还具有用于训练神经网络的若干其他期望属性。

![1560132778043](https://github.com/challow0/build_nn/blob/master/image/1.png)

**第05行**：请注意，此函数还可以生成sigmoid的导数（当deriv = True时）。 sigmoid函数的一个理想特性是它的输出可用于创建其导数。如果sigmoid的输出是变量“out”，那么导数就是 out *（1-out）。这非常有效。

如果您对导数不熟悉，只需将其视为给定点上S形函数的斜率（如上所示，不同点具有不同的斜率）。有关导数的更多信息，请查看Khan学院的导数教程。

**第10行**：这将我们的输入数据集初始化为numpy矩阵。每行都是一个“训练示例”。每列对应于我们的一个输入节点。因此，我们有3个网络输入节点和4个训练样例。

**第16行**：这将初始化我们的输出数据集。在这种情况下，我水平生成数据集（单行和4列）用于空间。 
“.T”是转置功能。转置后，该y矩阵有4行，一列。就像我们的输入一样，每一行都是一个训练示例，每一列（只有一行）是一个输出节点。因此，我们的网络有3个输入和1个输出。

**第20行**：种子随机数是一种好习惯。您的数字仍将随机分配，但每次训练时它们将以完全相同的方式随机分布。这样可以更轻松地查看更改对网络的影响。

**第23行**：这是我们的神经网络的权重矩阵。它被称为“syn0”意味着“突触零”。由于我们只有2层（输入和输出），我们只需要一个权重矩阵来连接它们。它的维数是（3,1）因为我们有3个输入和1个输出。查看它的另一种方法是l0的大小为3，l1的大小为1.因此，我们希望将l0中的每个节点连接到l1中的每个节点，这需要一个维度矩阵（3,1）。:)

还要注意它是随机初始化的，均值为零。有很多理论可以用于权重初始化。现在，只需将其作为最佳实践，在权重初始化中使用零均值是个好主意。

另一个注意事项是“神经网络”实际上只是这个矩阵。我们有“层”l0和l1但它们是基于数据集的瞬态值。我们不保存它们。所有学习都存储在syn0矩阵中。

**第25行**：这开始了我们实际的网络培训代码。这个for循环在训练代码上“迭代”多次，以优化我们的网络到数据集。

**第28行**：由于我们的第一层l0只是我们的数据。我们在这一点上明确地描述了它。请记住，X包含4个训练样例（行）。我们将在此实现中同时处理所有这些。这被称为“完整批次”培训。因此，我们有4个不同的l0行，但如果你愿意，你可以把它想象成一个单一的训练例子。在这一点上没有任何区别。
（如果我们想要，我们可以加载1000或10,000而不更改任何代码）。

**第29行**：这是我们的预测步骤。基本上，我们首先让网络“尝试”预测输入的输出。然后我们将研究它的执行方式，以便我们可以调整它以便为每次迭代做得更好。

这一行包含2个步骤。第一个矩阵将l0乘以syn0。第二个通过sigmoid函数传递输出。考虑每个的尺寸：
$$
(4 x 3) dot (3 x 1) = (4 x 1)
$$
矩阵乘法是有序的，因此等式中间的维度必须相同。因此，生成的最终矩阵是第一矩阵的行数和第二矩阵的列数。

由于我们加载了4个训练样例，我们最终得到4个正确答案的猜测，一个（4 x 1）矩阵。每个输出对应于网络对给定输入的猜测。也许它变得直观，为什么我们可以“加载”任意数量的训练样例。矩阵乘法仍然有效。 :)

**第32行**：所以，假设l1对每个输入都有一个“猜测”。我们现在可以通过从猜测（l1）中减去真实答案（y）来比较它的效果。 l1_error只是反映网络遗漏多少的正数和负数的向量。

**第36行**：现在我们正在寻找好东西！这是奥秘！这一行有很多，所以让我们进一步将它分成两部分。

#### First Part: The Derivative

```python
nonlin(l1,True)
```

如果l1代表这三个点，则上面的代码生成下面几行的斜率。请注意，非常高的值（例如x = 2.0（绿点））和非常低的值（例如x = -1.0（紫点））具有相当浅的斜率。您可以拥有的最高斜率是x = 0（蓝点）。这起着重要作用。还要注意所有导数都在0到1之间。

![1560133687937](https://github.com/challow0/build_nn/blob/master/image/2.png)

#### 整个声明：误差加权导数

```python
l1_delta = l1_error * nonlin(l1,True)
```

除了“误差加权导数”之外，还有更多“数学上精确”的方法，但我认为这可以捕捉到直觉。 l1_error是一个（4,1）矩阵。 nonlin（l1，True）返回一个（4,1）矩阵。我们正在做的是将它们“[元素化](https://nl.mathworks.com/help/matlab/ref/times.html)”。这返回带有乘法值的（4,1）矩阵**l1_delta**。

当我们将“斜率”乘以误差时，我们正在**减少高置信度预测的误差**。再看看sigmoid图片吧！如果斜率非常浅（接近0），那么网络要么具有非常高的值，要么具有非常低的值。这意味着网络对这种方式非常有信心。但是，如果网络猜到接近（x= 0，y = 0.5）的东西，那么它就不是很有信心。我们最重要地更新这些“多愁善感”的预测，并且我们倾向于通过将它们乘以接近0的数字来单独保留自信的预测。

**第39行**：我们现在准备更新我们的网络了！我们来看一个单一的训练示例。

![1560134764405](https://github.com/challow0/build_nn/blob/master/image/3.png)

在这个训练示例中，我们都设置了更新我们的权重。让我们更新最左边的权值（9.5）。

**weight_update = input_value \* l1_delta**

对于最左边的权重，这将乘以1.0 * l1_delta。据推测，这将略微增加9.5。为什么只有一个小的和？嗯，预测已经非常自信了，预测基本上是正确的。小错误和小斜率意味着**非常小**的更新。考虑所有权重。三者都会略微增加。

![1560134931257](https://github.com/challow0/build_nn/blob/master/image/4.png)

但是，因为我们正在使用“完整批量”构造，所以我们在所有四个训练示例上执行上述步骤。所以，它看起来更像上面的图像。那么，第39行做了什么？它计算每个训练样例的每个权重的权重更新，对它们求和，并更新权重，所有这些都在一个简单的行中。玩矩阵乘法，你会看到它做到这一点！

#### 小贴士：

因此，既然我们已经了解了网络的更新方式，那么让我们回顾一下我们的培训数据并进行反思。当输入和输出都是1时，我们增加它们之间的权重。当输入为1且输出为0时，我们减小它们之间的权重。

| Inputs |      |      | Output |
| :----: | :--: | :--: | :----: |
|   0    |  0   |  1   |   0    |
|   1    |  1   |  1   |   1    |
|   1    |  0   |  1   |   1    |
|   0    |  1   |  1   |   0    |

因此，在下面的四个训练示例中，从第一个输入到输出的权重将**一致地增加或保持不变**，而其他两个权重将在训练示例（取消进度）中发现它们**自身增加和减少**。这种现象是导致我们的网络基于输入和输出之间的相关性学习的原因。

## 第二部分：一个稍微难点的问题

| Inputs |      |      | Output |
| :----: | :--: | :--: | :----: |
|   0    |  0   |  1   |   0    |
|   0    |  1   |  1   |   1    |
|   1    |  0   |  1   |   1    |
|   1    |  1   |  1   |   0    |

考虑在给定两个输入列的情况下尝试预测输出列。一个关键的途径应该是两列都没有任何与输出的相关性。每列有50％的机会预测1和50％的机会预测0。

那么，模式是什么？它似乎与第三列完全无关，第三列始终为1.但是，第1列和第2列更清晰。如果第1列或第2列是1（但不是两个！），则输出为1.这是我们的模式。

这被认为是“非线性”模式，因为输入和输出之间没有直接的一对一关系。相反，**输入组合之间存在一对一的关系**，即第1列和第2列。 

![1560266608119](https://github.com/challow0/build_nn/blob/master/image/5.png)

信不信由你，图像识别是一个类似的问题。如果一个人拥有100个相同尺寸的烟斗和自行车图像，则没有单独的像素位置会直接与自行车或烟斗的存在相关联。从纯粹的统计角度来看，像素也可能是随机的。然而，**像素的某些组合**不是随机的，即形成自行车或人的图像的组合。

#### 我们的策略：

为了首先将像素组合成可以与输出具有一对一关系的东西，我们需要添加另一个层。我们的第一层将组合输入，然后我们的第二层将使用第一层的输出作为输入将它们映射到输出。在我们进入实现之前，请看一下这个表。

| Inputs (l0) |      |      | Hidden Weights (l1) |      |      |      | Output (l2) |
| :---------: | :--: | :--: | :-----------------: | ---- | ---- | ---- | :---------: |
|      0      |  0   |  1   |         0.1         | 0.2  | 0.5  | 0.2  |      0      |
|      0      |  1   |  1   |         0.2         | 0.6  | 0.7  | 0.1  |      1      |
|      1      |  0   |  1   |         0.3         | 0.2  | 0.3  | 0.9  |      1      |
|      1      |  1   |  1   |         0.2         | 0.1  | 0.3  | 0.8  |      0      |

如果我们随机初始化我们的权重，我们将获得第1层的隐藏状态值。注意到了什么？**第二列（第二个隐藏节点）已经与输出略有关联！**它并不完美，但它就在那里。信不信由你，这是神经网络训练的重要组成部分。 （可以说，这是神经网络训练的唯一方式。）下面的训练将放大这种相关性。它将更新syn1以将其映射到输出，并更新syn0以更好地从输入生成它！

注意：添加更多层以模拟更多关系组合的领域（例如这种情况）被称为“深度学习”，因为建模的层越来越深。

#### 3层神经网络：

```python
import numpy as np

def nonlin(x,deriv=False):
	if(deriv==True):
	    return x*(1-x)

	return 1/(1+np.exp(-x))
    
X = np.array([[0,0,1],
            [0,1,1],
            [1,0,1],
            [1,1,1]])
                
y = np.array([[0],
			[1],
			[1],
			[0]])

np.random.seed(1)

# 用均值0随机初始化我们的权重
syn0 = 2*np.random.random((3,4)) - 1
syn1 = 2*np.random.random((4,1)) - 1

for j in xrange(60000):

	# 通过层0,1和2前馈
    l0 = X
    l1 = nonlin(np.dot(l0,syn0))
    l2 = nonlin(np.dot(l1,syn1))

    # 我们失去了多少目标值？
    l2_error = y - l2
    
    if (j% 10000) == 0:
        print ("Error:" + str(np.mean(np.abs(l2_error))))
        
    # 哪个方向是目标值？
    # 我们真的很确定吗？如果是这样，不要改变太多。
    l2_delta = l2_error*nonlin(l2,deriv=True)

    # 每个l1值对l2误差贡献了多少（根据权重）？
    l1_error = l2_delta.dot(syn1.T)
    
    # 目标l1在哪个方向？
    # 我们真的很确定吗？如果是这样，不要改变太多。
    l1_delta = l1_error * nonlin(l1,deriv=True)

    syn1 += l1.T.dot(l2_delta)
    syn0 += l0.T.dot(l1_delta)
	'''
	Error:0.496410031903
	Error:0.00858452565325
	Error:0.00578945986251
	Error:0.00462917677677
	Error:0.00395876528027
	Error:0.00351012256786
	'''
```

| Variable | Definition                                                   |
| :------: | ------------------------------------------------------------ |
|    X     | 输入数据集矩阵，其中每一行都是一个训练示例                   |
|    y     | 输出数据集矩阵，其中每一行都是一个训练示例                   |
|    l0    | 网络的第一层，由输入数据指定                                 |
|    l1    | 网络的第二层，也称为隐藏层                                   |
|    l2    | 网络的最后一层，这是我们的假设，在我们训练时应该接近正确的答案。 |
|   syn0   | 第一层权重，Synapse 0，连接l0到l1.                           |
|   syn1   | 第二层重量，Synapse 1连接l1到l2.                             |
| l2_error | 这是神经网络“错过”的数量                                     |
| l2_delta | 这是由置信度缩放的网络错误。除了非常明显的错误被排除之外，它几乎与错误完全相同。 |
| l1_error | 通过syn1中的权重加权l2_delta，我们可以计算中间/隐藏层中的误差。 |
| l1_delta | 这是由置信度缩放的网络的l1错误。同样，它与l1_error几乎相同，只是明显错误被排除。 |

一切都应该看起来很熟悉！它实际上只是之前实现中的两个相互叠加。第一层（l1）的输出是第二层的输入。这里发生的唯一新事物是第43行。

**第43行**：使用来自l2的“置信度加权误差”来为l1建立误差。这样做，它只是通过权重将误差从l2送到到l1。这给出了你可以称之为“贡献加权误差”的东西，因为我们知道l1中的每个节点值对l2中的错误“贡献”了多少。此步骤称为“反向传播”，是算法的同名。然后，我们使用与2层实现中相同的步骤更新syn0。